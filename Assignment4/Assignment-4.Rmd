
---
title: "Assignment 4"
author: "Tanjina Islam, Miguel Morales Expósito and Carlos Perales Liñan, group 12"
date: "22 March 2018"
output:
  pdf_document:
    fig_caption: yes
fontsize: 11pt
highlight: tango
---
```{r,echo=FALSE, warning=FALSE,results='hide',message=FALSE}
options(digits = 3) #Showing only 3 decimals
library(multcomp)
library(lme4)
```


## Exercise 1

We load the data from the data source.
```{r}
flies = read.table("fruitflies.txt",header=TRUE);
```

### Task 1 

We add the logarithm of longevity in the data frame.

```{r}
fliesframe = data.frame(thorax=flies$thorax,longevity=flies$longevity,activity=flies$activity,loglongevity=log(flies$longevity))
```


### Task 2

```{r}
hist(fliesframe$thorax)
qqnorm(fliesframe$thorax)
```
The population of thorax does not seem normal.

```{r}
hist(fliesframe$loglongevity)
qqnorm(fliesframe$loglongevity)
```
The population of loglongevity does not seem normal.


### Task 3

We perfom an anova just considering the sexual activity
```{r}
fliesanova = lm(loglongevity~activity,data=fliesframe)
anova(fliesanova)
```
We get a p-value 1.798e-07, therefore, H0 is reject so we can say that sexual activity affects the longevity of the flies.


### Task 4

```{r}
summary(fliesanova)
```
According to the estimates longevity increases more when the sexual activity is low. 
..* High activity =  3.602
..* Low activity =  3.602 + 0.517 = 4.119
..* Isolated =  3.602 + 0.397 = 3.999


### Task 5

We make a 2-way anova.
```{r}
fliesframe$activity = as.factor(fliesframe$activity)
fliesframe$thorax = as.factor(fliesframe$thorax)

fliesanova2 = lm(loglongevity~activity+thorax,data = fliesframe) 
anova(fliesanova2)
```
We obtain a p-value of 1.141e-13, therefore, we can say that activity has a main effect on longevity when we consider the thorax length.

### Task 6

First we calculate the average thorax length.
```{r}
fliesframe$thorax = as.numeric(fliesframe$thorax)
average_thorax = mean(fliesframe$thorax) 
fliesframe$thorax = as.factor(fliesframe$thorax)

```

We obtain an average of `r average_thorax`.

We obtain the summary.
```{r}
contrasts(fliesframe$thorax)=contr.sum
contrasts(fliesframe$activity)=contr.sum
fliesanova2 = lm(loglongevity~activity+thorax,data = fliesframe) 
summary(fliesanova2)
```
With the values thorax9 = 0.312 (for the average thorax length) and isolated-activity = -0.275, low-activity = 0.186 and high-activity = -(-0.275+0.186) = 0.089, we calculate the estimates for flies with average thorax..

Y~isolated,thorax9~ = 3.802 + 0.312 - 0.275 = 3.839
Y~low,thorax9~ = 3.802 + 0.312 + 0.186 = 4.3
Y~high,thorax9~ = 3.802 + 0.312 + 0.089 = 4.203

For the flies with smallest thorax we use the same estimates for activity but we use thorax1 = -0.545.

Y~isolated,thorax1~ = 3.802 - 0.545 - 0.275 = 2.982
Y~low,thorax1~ = 3.802 - 0.545 + 0.186 = 3.443
Y~high,thorax1~ = 3.802 - 0.545 + 0.089 = 3.346


### Task 7

In order to investigate graphically how does thorax length influence longevity, we separate in three different variables the data depending on the sexual activity.
```{r}
fliesframe$thorax = as.numeric(fliesframe$thorax)
flieshigh = fliesframe[which(fliesframe$activity=="high"),]
fliesisolated = fliesframe[which(fliesframe$activity=="isolated"),]
flieslow = fliesframe[which(fliesframe$activity=="low"),]
```

Now we plot the thorax length against the loglongevity.
```{r}
plot(flieshigh$thorax,flieshigh$loglongevity,main="High activity with loglongevity")
plot(fliesisolated$thorax,fliesisolated$loglongevity,main="Isolated activity with loglongevity")
plot(flieslow$thorax,flieslow$loglongevity,main="Low activity with loglongevity")
```
We see that longevity increases with thorax length. We get higher longevity values for flies with low sexual activity and also for high sexual activity. The values seem to be lower when the flies have been isolated. 

### Task 8

The analysis with the thorax length should not be included because there is no real interaction between the sexual activity and the thorax length. 


This is because the experimenters randomly chose the sexual activity that the flies were going to have. Thus, we cannot conclude whether the thorax length will really influence in the sexual activity of the fly.


### Task 9

```{r}

qqnorm(residuals(fliesanova2))
```
The normality seems doubtful according to the qq-plot.

```{r}
plot(fitted(fliesanova2),residuals(fliesanova2))
```
The spread in the residuals seem to be bigger with bigger fitted values.

### Task 10

```{r}
fliesframe$activity=as.factor(fliesframe$longevity)
fliesaov = lm(longevity~thorax+activity,data=fliesframe)
drop1(fliesaov,type="F")
```


```{r}
qqnorm(residuals(fliesanova))
qqline(residuals(fliesanova))
```
The normality is doubtful. However, it could be normal. It seems more normal than when using the logaritmic value of longevity.

```{r}
plot(fitted(fliesanova),residuals(fliesanova))
```
We cannot judge becasue the values are not spread in the x axis. This is a prove that using the logarithmic value was a good idea.



## Exercise 2

We load the data from the data source.
```{r}
expsi = read.table("psi.txt", header=TRUE)

```

### Task 1 

To help us on making some summaries, we divide the data in 2 groups: the students that received psi and the ones that didn't.
```{r}
nousepsi = expsi[which(expsi$psi == "0"),]
usepsi = expsi[which(expsi$psi == "1"),]

```

Next, we show some data summaries:
```{r}

hist(nousepsi$gpa, breaks = c(2.0, 2.5, 3.0, 3.5, 4.0, 4.5), xlab="GPA", ylab="N of students", main="GPA of students not receiving PSI")

hist(usepsi$gpa, breaks = c(2.0, 2.5, 3.0, 3.5, 4.0, 4.5), xlab="GPA", ylab="N of students", main="GPA of students receiving PSI")

boxplot(expsi$gpa, usepsi$gpa, nousepsi$gpa, main="GPA Boxplots", names=c("All", "PSI", "No PSI"));

qqnorm(nousepsi$gpa,  main="Q-Q Plot of Student's GPA without PSI") # doesn't look normal
qqline(nousepsi$gpa, col="red")

qqnorm(usepsi$gpa, main="Q-Q Plot of Student's GPA receiving PSI") # looks normal
qqline(usepsi$gpa, col="red")
```


### Task 2

We fit them into a linear regression model.

```{r}
expsiglm=glm(passed~psi+gpa,data=expsi,family=binomial)
summary(expsiglm)

```


### Task 3

As we can see in the summary of the linear model, the usage of psi increases the linear predictor by 2.338.

Knowing this, we can calculate that it increases odds of passing by exp^2.338 = 10.3604948382


### Task 4

With the summary from task 2, we can see the values that we need to estimate these probabilities.

*With psi and gpa = 3.0:* 

-11.602 + 2.338 + (3.063 * 3.0) = -0.075 
Probability = 1/(1 + e^0.075) = 0.4812588


*Without psi and gpa = 3.0:* 

-11.602 + (3.063 * 3.0) = -2.408 
Probability = 1/(1 + e^2.408) = 0.08256469



### Task 5

We can also get these probabilities with the help of the summary in task 2.

*With psi:* 

Probability = 1/(1 + e^11.602) = 9.15e-06


*Without psi:* 

-11.602 + 2.34 = -9.26 
Probability = 1/(1 + e^9.26) = 9.51e-05


*Difference of probabilities:* 9.51e-05 - 9.15e-06 = 8.59e-05



### Task 6

```{r}
x=matrix(c(3,15,8,6),2,2)
x
```

The number 15 are the students that didn't show improvement from the 18, whereas 3 are those who did show improvement. Similarly with the second column, only 8 out of 14 students showed improvement and the remaining 6 didn't.

With the observations above, we can claim that the first column contains the students not receiving psi and the second one shows the students receiving it. Furthermore, row 1 shows the students that improved and row 2 the number of students that didn't.

```{r, results='hide'}
fisher.test(x)

```

After running Fisher's test, we can conclude that with a p-value of [`r fisher.test(x)[[1]]`], we can reject the null hypothesis, which claims that the students receiving psi and the ones not receiving it have the same probablity of improvement.


### Task 7

We don't believe this approach is wrong, Fisher's test is a right approach for 2x2 tables with not too big numbers. We believe that this approach is valid since the experiment meet the requirements for its use.


### Task 8

Fishers is good for small counts in each 2x2 table cell. 
More exact than the other approach, but it doesn't work with  big counts.



## Exercise 3

First of all, we load the data from the data source.
```{r}
africa_data = read.table("africa.txt", header = TRUE)

```

### Task 1 

We are going to create an array with 12 different poisson distributions. The distributions are combinations of the values n={10,100,1000} with lambda values l={0.5,1,10,100}.

```{r}
n = c(10, 100, 1000)
lambda = c(0.5, 1, 10, 100)

poisson_st = list()

for(i in n){
  for(j in lambda){
    poisson_st = c(poisson_st,list(rpois(i,j)))
  }
}

```


```{r}
x = poisson_st[[1]]
hist(x, main = "Histogram with n = 10 and lambda = 0.5")
qqnorm(x, main = "n = 10 and lambda = 0.5")
```

The distribution doesn't look normal.

```{r}
x = poisson_st[[2]]
hist(x, main = "Histogram with n = 10 and lambda = 1")
qqnorm(x, main = "n = 10 and lambda = 1")

```
The distribution doesn't look normal.



```{r}
x = poisson_st[[3]]
hist(x, main = "Histogram with n = 10 and lambda = 10")
qqnorm(x, main = "n = 10 and lambda = 10")
```
The distribution doesn't look normal.


```{r}
x = poisson_st[[4]]
x
hist(x, main = "Histogram with n = 10 and lambda = 100")
qqnorm(x, main = "n = 10 and lambda = 100")
qqline(x, col= "red")
```
The histogram and QQ-plot resemble normality.


```{r}
x = poisson_st[[5]]
hist(x, main = "Histogram with n = 100 and lambda = 0.5")
qqnorm(x, main = "n = 100 and lambda = 0.5")
```
The distribution doesn't look normal.


```{r}
x = poisson_st[[6]]
hist(x, main = "Histogram with n = 100 and lambda = 1")
qqnorm(x, main = "n = 100 and lambda = 1")
```
The distribution doesn't look normal.


```{r}
x = poisson_st[[7]]
hist(x, main = "Histogram with n = 100 and lambda = 10")
qqnorm(x, main = "n = 100 and lambda = 10")
```

The histogram could be close to normal but the QQ-plot is not.



```{r}
x = poisson_st[[8]]
hist(x, main = "Histogram with n = 100 and lambda = 100")
qqnorm(x, main = "n = 100 and lambda = 100")
```

The histogram looks normal but the QQ-plot seems to follow  a stepped pattern.

```{r}
x = poisson_st[[9]]
hist(x, main = "Histogram with n = 1000 and lambda = 0.5")
qqnorm(x, main = "n = 1000 and lambda = 0.5")
```
The distribution doesn't look normal.


```{r}
x = poisson_st[[10]]
hist(x, main = "Histogram with n = 1000 and lambda = 1")
qqnorm(x, main = "n = 1000 and lambda = 1")
```
The distribution doesn't look normal.


```{r}
x = poisson_st[[11]]
hist(x, main = "Histogram with n = 1000 and lambda = 10")
qqnorm(x, main = "n = 1000 and lambda = 10")
```

Histogram shows normality but the QQ-plot shows a stepped pattern.

```{r}
x = poisson_st[[12]]
hist(x, main = "Histogram with n = 1000 and lambda = 100")
qqnorm(x, main = "n = 1000 and lambda = 100")
```
Both histogram and QQ-plot show normality.

#### Findings

We can see from the previous histograms and QQ-plots that, when increasing the number of samples and lambda values of the Poisson distribution, the histograms look aproximately equal to normal distributions. However, looking at the QQ-plot we see that for some combinations of n and lambda (i.e n=10 l=100) they look normal but in most of the other combinations we see a stepped pattern. 

 

### Task 2

The mean and variance of the Poisson distribution both equal lambda. Hence, the larger the parameter, the larger the values of Y on average and the larger the spread in the values of Y.

If we look into the Poisson distributions that we generated above in task 1, we can see that for larger lambda values we get a distribution which seems more like a normal distribution.

In poisson regression, for each observation Y the parameter lambda is modelled differently, since the corresponding values will differ in general. The variances are different as well.

Therefore, we believe that they're not in the same location-scale family.



### Task 3

We perform Poisson regression on the data.
```{r}
africa_glm = glm(miltcoup~oligarchy+pollib+parties+pctvote+popn+size+numelec+numregim, family = poisson, data = africa_data)
summary(africa_glm)

```


### Task 4

Using the step-down approach, we will reduce the number of variables in our model.
```{r}
africa_glm_sd = glm(miltcoup~oligarchy+pollib+parties+pctvote+popn+size+numelec+numregim, family = poisson, data = africa_data)
summary(africa_glm_sd)

summary(africa_glm_sd)[[12]][35]

```
As we can see, the variable numelec has the highest p-value and since it is > 0.05, we discard it for the next iteration.


```{r}
africa_glm_sd = glm(miltcoup~oligarchy+pollib+parties+pctvote+popn+size+numregim, family = poisson, data = africa_data)
summary(africa_glm_sd)

summary(africa_glm_sd)[[12]][32]
```
In this iteration, numregim has the highest p-value and it is > 0.05. Hence, we discard it for the next iteration.


```{r}
africa_glm_sd = glm(miltcoup~oligarchy+pollib+parties+pctvote+popn+size, family = poisson, data = africa_data)
summary(africa_glm_sd)

summary(africa_glm_sd)[[12]][28] 
```
Here, size has the highest p-value . As we can see, this value is > 0.05. This means we will discard it for our model and proceed to the next iteration.


```{r}

africa_glm_sd = glm(miltcoup~oligarchy+pollib+parties+pctvote+popn, family = poisson, data = africa_data)
summary(africa_glm_sd)

summary(africa_glm_sd)[[12]][24]
```
We pick the highest p-value, the one of the variable popn. This value is > 0.05, so discard it for the next iteration.


```{r}
africa_glm_sd = glm(miltcoup~oligarchy+pollib+parties+pctvote, family = poisson, data = africa_data)
summary(africa_glm_sd)

summary(africa_glm_sd)[[12]][20]
```
Of all the variables, pctvote has the highest p-value. It is > 0.05. So we discard it for the next iteration.


```{r}
africa_glm_sd = glm(miltcoup~oligarchy+pollib+parties, family = poisson, data = africa_data)
summary(africa_glm_sd)
```
As we can see, all the p-values are now smaller than 0.05. Meaning that all the variables are significant for our model.


The resulting model of the step-down method is:

miltcoup = 0.25138 + 0.09262*oligarchy - 0.57410*pollib + 0.02206*parties + error


### Task 5

Next, we will show some diagnostic plots for our model.


#### Plots: Fitted vs Residuals

```{r}
plot(fitted(africa_glm_sd), residuals(africa_glm_sd)) 

```
We can't recognize any specific pattern. Data is scattered, and it's not visually good as it would suppose to be in linear regression model.

Because of this, we will take the logarithm to make the x-values fitted by a linear function in the plot. 


#### Plots: Logarithmic-Fitted values vs Residuals

```{r}
plot(log(fitted(africa_glm_sd)), residuals(africa_glm_sd))

```
The plot seems OK yet still no specific pattern. And still looks scattered.


#### Plots: Logarithmic-Fitted values vs Residuals with type = Response

```{r}
plot(log(fitted(africa_glm_sd)), residuals(africa_glm_sd, type = "response"))

```

###### Check Lec 10 slide: 41-43


#### Plots: Fitted vs Residuals in the Full Model

```{r}
plot(fitted(africa_glm), residuals(africa_glm))

```

#### Plots: Logarithmic-Fitted values vs Residuals in the Full Model

```{r}
plot(log(fitted(africa_glm)), residuals(africa_glm))

```

#### Plots: Logarithmic-Fitted values vs Residuals with type = Response in the Full Model

```{r}
plot(log(fitted(africa_glm)), residuals(africa_glm, type = "response"))

```
The plots look scattered. All of them follow a certain pattern in our model from task-4. After ploting using the full model, we found the same pattern there too.

Therefore, we can discard that the readon is that we deleted too many variables.

Now we will check normality assumption.
```{r}

shapiro.test(residuals(africa_glm_sd)) # p-value = 0.01 < 0.05 # Normality is doubtful

shapiro.test(residuals(africa_glm)) # p-value = 0.01 < 0.05 # Normaility is doubtful
```

```{r}
qqnorm(residuals(africa_glm_sd)) # doesn't seem normal
qqline(residuals(africa_glm_sd), col="red")
```

```{r}

qqnorm(residuals(africa_glm)) # doesn't seem normal
qqline(residuals(africa_glm), col="red")
```

In both models we got the same results. The plots that we generated for the model that we found in task-4 and for the full model with all explanatory variables followed same type of pattern. For these reasons, the assumption of normality (if any) is doubtful.

This means that the sample might not come from a normal distribution.



