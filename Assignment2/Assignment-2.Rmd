---
title: "Assignment 2"
author: "Tanjina Islam, Miguel Morales Expósito and Carlos Perales Liñan, group 12"
date: "28 February 2018"
output:
  pdf_document:
    fig_caption: yes
    latex_engine: xelatex
fontsize: 11pt
highlight: tango
---

## Exercise 1
### Task 1
We load the data from the data source.
```{r}
Data <- read.table("telephone.txt",header = TRUE);
bills = Data$Bills;
```
We define our t statistics which is going to be the median. 
```{r}
t=median(bills);
n=length(bills);

```

We define the function for the bootstrap test for different lambda values.
```{r}
bootstrap_exp<-function(n,lambda){
  B=1000;
  tstar=numeric(B);
  for(i in 1:B){
    xstar = rexp(n,lambda); #Exercise says for lambda = [0.01,0.1]
    tstar[i] = median(xstar);
  }
  return(tstar)
}
```
We define 200 hundred values for lambda and perform the bootstrap test on each of them.
```{r}
lambda_values=seq(0.01,0.1,length=200)
p_array=numeric(length(lambda_values))
B=1000;
i=1
for(lambda in lambda_values){
  tstar=bootstrap_exp(n,lambda);
  pl=sum(tstar<t)/B;
  pr=sum(tstar>t)/B;
  p=2*min(pl,pr);
  p_array[i]=p;
  i=i+1;
}
```
We take the values where p is not rejected
```{r}
p_not_reject=p_array[p_array>0.05]
```
We get the corresponding lambda values for those p-values
```{r}
indexes = match(p_not_reject,p_array);
lambda_not_reject=numeric(length(indexes));
count=1
for(i in indexes){
  lambda_not_reject[count]=lambda_values[i];
  count=count+1;
}
```
We cannot reject that the data stems from a exponential distribution with lambda values: `r lambda_not_reject `

```{r}
plot(lambda_values,p_array)
```


### Task 2 

```{r}
hist(bills)
boxplot(bills)
qqnorm(bills)
```

Considering that the median of the data is `r median(bills)` the marketing manager could focus the marketing campaign saying that half of their customers spend less than `r median(bills)` euros every month.

## Exercise 2
We load the data and normalize it so the data represents the same from the different data sources
```{r}
light1879 = scan("light1879.txt");
light1882 = scan("light1882.txt")
light_time = scan("light.txt") #Holds the times saved from the experiment which are NOT the seconds taken to travel 7442Km

light_real_time = ((light_time/1000)+24.8)/1000000 #Holds the real time taken to travel 7442Km in seconds
light_speed = (7.442/light_real_time)-299000 #Measured speed of light minus 299000
```

### Taks 1 
Exercise 2: Task-1
```{r}
hist(light1879);
boxplot(light1879);
```
```{r}
hist(light1882);
boxplot(light1882);

```

```{r}
hist(light_speed);
boxplot(light_speed);
```

```{r}
boxplot(light1879,light1882,light_speed,names=c("Light 1879","Light 1882","Light Newcomb"))
```

Comparing the histograms and boxplots we can see that the distribution of the light1882 and light_speed are quite similar. Meanwhile, the light1879 the boxplot seems shifted upwards which means that the values are bigger.
If we consider the median values then we get for light1879 is `r median(light1879)`, for 1882 is `r median(light1882)` and light_speed is `r median(light_speed)`. We see again that light1882 and light_speed medians are close to each other.


### Task 2
We define a function to perform a bootstrap test
```{r}
bootstrap<-function(data,operation="mean"){
  B=1000;
  tstar=numeric(B);
  
  for(i in 1:B){
    xstar = sample(data,replace = TRUE);
    
    if(operation=="mean"){
      tstar[i] = mean(xstar);
    }else if(operation=="median"){
      tstar[i] = median(xstar);
    }
  }
  return(tstar);
}
```
We define a function to get the confidence intervals
```{r}
get_confidence_intervals<-function(data,operation="mean"){
  if(operation=="mean"){
    t=mean(data);
  }else if(operation=="median"){
    t=median(data);
  }
  tstar = bootstrap(data,operation);
  tstar25 = quantile(tstar,0.025);
  tstar975 = quantile(tstar,0.975);
  return(c(2*t-tstar975,2*t-tstar25));
}
```
#### Light 1879
Confidence interval for the mean
```{r}
confidence_mean = get_confidence_intervals(light1879);
confidence_mean = confidence_mean+299000; #We add 299000 to get the actual speed
```
The confidence interval for the mean is `r confidence_mean`

Confidence interval for the median
```{r}
confidence_median = get_confidence_intervals(light1879,"median");
confidence_median = confidence_median+299000;
```
The confidence interval for the median is `r confidence_median`

#### Light 1882
Confidence interval for the mean
```{r}
confidence_mean = get_confidence_intervals(light1882);
confidence_mean = confidence_mean+299000; #We add 299000 to get the actual speed
```
The confidence interval for the mean is `r confidence_mean`

Confidence interval for the median
```{r}
confidence_median = get_confidence_intervals(light1882,"median");
confidence_median = confidence_median+299000;
```
The confidence interval for the median is `r confidence_median`

#### Light Newcomb
Confidence interval for the mean
```{r}
confidence_mean = get_confidence_intervals(light_speed);
confidence_mean = confidence_mean+299000; #We add 299000 to get the actual speed
```
The confidence interval for the mean is `r confidence_mean`

Confidence interval for the median
```{r}
confidence_median = get_confidence_intervals(light_speed,"median");
confidence_median = confidence_median+299000;
```
The confidence interval for the median is `r confidence_median`

### Task 3
Looking at the confidence intervals of light1882 and light Newcomb, we can find that they are similar and more or less are in the same range. Meanwhile, the confidence interval of 1879 is not in the same range as the other two for the mean and for the median. The intersection of the intervals of 1879 and the others is empty.
Then the experiments of 1879 maybe are not accurate enough or at least they differ from the other two experiments.

### Task 4
The speed of light according to https://physics.nist.gov/cgi-bin/cuu/Value?c is 299792.458.

Checking the intervals in task2, we can see that the value of the speed of light only falls in the interval for the mean and median of the speed light of 1882.


## Exercise 3
We load the data from the data source.
```{r}
klm = scan("klm.txt");
```
```{r}
hist(klm)
qqnorm(klm)
qqline(klm)
```
We check that the population does not seem normal. This tells us that doing the t-test is not the best option.
We should perform the sign test.
Wilcoxon test does not seem appropiate because the population is not symmetric.
### Task 1
We compute how many values are bigger than 31 and perform a binom test with that data.
```{r, results="hide"}
n_bigger = sum(klm>31)
n = length(klm)
binom.test(n_bigger,n,p=0.5,alternative = "greater")
```
We obtain a p-value of `r binom.test(n_bigger,n,p=0.5,alternative = "greater")[[3]]` so we can reject the null hypothesis that the median of the population is smaller or equal than 31. 
### Task 2
We compute how many values are bigger than 72. We perform a binomial test with p = 0.1.
```{r, results="hide"}
n_bigger = sum(klm>72);
binom.test(n_bigger,n,p=0.1,alternative = "greater")
```
With a p value of `r binom.test(n_bigger,n,p=0.1,alternative = "greater")[[3]]` we can reject that at most 10% of the parts arrive later than 72 days.

## Exercise 4
We load the data from the data source.
```{r}
Clouds<-read.table("clouds.txt",header = TRUE);
```

### Task 1

#### T-test
We check if the differences of the data come from a normal population.
```{r}
differences=Clouds$seeded-Clouds$unseeded;
qqnorm(differences);
qqline(differences);
```
It seems that it does not.

We perform the t-test.
```{r,results = "hide"}
t.test(Clouds$seeded,Clouds$unseeded,paired = TRUE)
```
The p-value `r t.test(Clouds$seeded,Clouds$unseeded,paired = TRUE)[[3]]` indicates that the means of both populations are different so the nitrate has an effect.

####  Mann-Whitney-Wilcoxon
We perform the test.
```{r,results="hide"}
wilcox.test(Clouds$seeded,Clouds$unseeded)
```
The p-value `r wilcox.test(Clouds$seeded,Clouds$unseeded)[[3]]` indicates that the two samples come from different populations. Therefore, the nitrate has an effect.

####  Kolmogorov-Smirnov
We perform the test.
```{r,results="hide"}
ks.test(Clouds$seeded,Clouds$unseeded);
```
The p-value `r ks.test(Clouds$seeded,Clouds$unseeded)[[2]]` indicates that the two samples come from different populations. Therefore, the nitrate has an effect.

#### Findings
The t-test seems appropiate becuase the data is paired but when we check the normality in the differences, it appears that the distribution is not normal so the t-test is doubtfull. Since the size of the sample is small maybe it could be the reason why the distribution does not seem normal.

Mann-Whitney and Kolmogorov-Smirnov do not seem appropiate because those test are ment to be use when the data is independent but they are not.

### Task 2
We assign the square-root values of clouds to a new variable.
```{r}
clouds_sq=sqrt(Clouds)
```

We perform the test on this variable.

```{r,results="hide"}
t.test(clouds_sq$seeded,clouds_sq$unseeded,paired = TRUE)
```
 The p-value `r t.test(clouds_sq$seeded,clouds_sq$unseeded,paired = TRUE)[[3]]` indicates that we can reject the null hypothesis.
 
```{r}
wilcox.test(clouds_sq$seeded,clouds_sq$unseeded)
```
The p-value `r wilcox.test(clouds_sq$seeded,clouds_sq$unseeded)[[3]]` indicates that we can reject the null hypothesis.

```{r, results="hide"}
ks.test(clouds_sq$seeded,clouds_sq$unseeded)
```
The p-value`r ks.test(clouds_sq$seeded,clouds_sq$unseeded)[[2]]` indicates that we can reject the null hypothesis.

### Findings :
Applying square root to the samples, when we perform t-test we can see that the p-value is being decreased.
While on the other hand, in both Mann-whitney-wilcoxon test and Kolmorov-sminorv test p-value remains the same for the square rooted values.
 
 
### Task 3
We assign the square-root values of the square-root of clouds to a new variable. 

```{r}
clouds_sq_sq=sqrt(clouds_sq)
```

We perform the tests on this variable.

```{r, results="hide"}
t.test(clouds_sq_sq$seeded,clouds_sq_sq$unseeded,paired = TRUE)
```
The p-value `r t.test(clouds_sq_sq$seeded,clouds_sq_sq$unseeded,paired = TRUE)[[3]]` indicates that we can reject the null hypothesis, H0.

```{r, results="hide"}
wilcox.test(clouds_sq_sq$seeded,clouds_sq_sq$unseeded)
```
The p-value `r wilcox.test(clouds_sq_sq$seeded,clouds_sq_sq$unseeded)[[3]]` indicates that we can reject our null hypothesis, Ho.

```{r}
ks.test(clouds_sq_sq$seeded,clouds_sq_sq$unseeded)
```

The p-value `r ks.test(clouds_sq_sq$seeded,clouds_sq_sq$unseeded)[[2]]` indicates that we can reject our null hypothesis, Ho.

### Findings :
Applying square root to the square-rooted values of the samples, when we perform t-test we can see that again the p-value is being decreased.
While on the other hand, in both Mann-whitney-wilcoxon test and Kolmorov-sminorv test p-value remains the same for the square rooted values.

## Exercise 5
We load the data from the data source.
```{r}
peruvians = read.table("peruvians.txt", header = TRUE);
peruvians = peruvians[, - c(5, 6, 7)] # Deleting redundant columns since we don't need to use them for this exercise
attach(peruvians)
```

### Task 1
We make the plot of each pair of two variables.
```{r}
pairs(peruvians)
```
From this plot we can see that migration is probably correlated with age and weight because when the values of migration increase, then the values of age and weight also increase.
### Task 2

First, we check the normality of variable migration.After finding the normality then we can decide which test should we perform for corelation checking.

```{r, echo=FALSE}

qqnorm(migration, main = "QQ - Plot of Migration")

```
```{r, results="hide"}

shapiro.test(migration)
```
The shapiro test gives a p-value of `r shapiro.test(migration)[[3]]` which rejects the null hypothesis that the distribution is normal.
From the above test result and from QQ-Plot, we found that migration does not belong to normal distribution. Therefore, we will use the rank correlation test of Spearman.

```{r, results="hide"}
cor.test(migration, age, method = "spearman")
```
The p-value `r cor.test(migration, age, method = "spearman")[[3]]` indicates that the null hypothesis that the correlation is 0 should be rejected. Therefore, migration and age are correlated.

```{r, results="hide"}
cor.test(migration, weight, method = "spearman")
```
The p-value `r cor.test(migration, weight, method = "spearman")[[3]]` indicates that the null hypothesis that the correlation is 0 should be rejected. Therefore, migration and weight are correlated.

```{r, results="hide"}
cor.test(migration, length, method = "spearman")
```
The p-value `r cor.test(migration, length, method = "spearman")[[3]]` indicates that  we cannot reject that the null hypothesis that the correlation is 0. Therefore, migration and length are not correlated.

```{r, results="hide"}
cor.test(migration, wrist, method = "spearman")
```
The p-value `r cor.test(migration, wrist, method = "spearman")[[3]]` indicates that  we cannot reject that the null hypothesis that the correlation is 0. Therefore, migration and heart rate are not correlated.

```{r, results="hide"}
cor.test(migration, systolic, method = "spearman")
```
The p-value `r cor.test(migration, systolic, method = "spearman")[[3]]` indicates that  we cannot reject that the null hypothesis that the correlation is 0. Therefore, migration and systolic are not correlated.

```{r, results="hide"}
cor.test(migration, diastolic, method = "spearman")
```
The p-value `r cor.test(migration, diastolic, method = "spearman")[[3]]` indicates that  we cannot reject that the null hypothesis that the correlation is 0. Therefore, migration and diastolic are not correlated.

## Exercise 6

First we get the data from the data source
```{r}
run = read.table("run.txt", header = TRUE)
attach(run)
```

### Task 1
Here we show histogram and qqplot of the data before taking the drinks.
```{r}

hist(before)
qqnorm(before)
qqline(before) 
```

Followed by the same kind of plots of the data after taking the drinks.
```{r}
hist(after)
qqnorm(after)
qqline(after) 
```

Then we plot both data together in order to be able to see the differences.
```{r}

plot(before, after, main = "Before Vs After")
abline(lm(after~before), col="red")
boxplot(before, after, names = c("before", "after"))
```


Finally we plot the differences of the two datasets in a histogram and qqplot.

```{r}

hist(before-after)
qqnorm(before-after)
qqline(before-after) 
boxplot(before-after, names = "before - after")
```


### Task 2
Since t-test runs under the assumption that the differences of the samples come from a normal distribution, we check the normality using the shapiro test.
```{r}
shapiro.test(before-after)

```
The test proves the assumption that the difference is normal.

Now we test the data before having the soft drink with the one after.
```{r,results="hide"}
lemo = run[which(drink == "lemo"),]
energy = run[which(drink == "energy"),]
t.test(lemo$before, lemo$after, paired = TRUE) 

```
Having the p-value `r t.test(lemo$before, lemo$after, paired = TRUE)[[3]]` we cannot reject that the mean of the differences is 0. Therefore, we can conclude the soft drinks do not affect the runners.

Now we do the same test for the energy drinks.
```{r}
t.test(energy$before, energy$after, paired = TRUE)
```

Having the p-value `r t.test(energy$before, energy$after, paired = TRUE)[[3]]` we cannot reject that the mean of the differences is 0. Therefore, we can conclude the energy drinks do not affect the runners.



### Task 3

Now we test individually if the time differences are affected by the drink type.
```{r}
lemo_time_difference = lemo$before - lemo$after

energy_time_difference = energy$before - energy$after

t.test(lemo_time_difference, energy_time_difference)

```
Having the p-value `r t.test(lemo_time_difference, energy_time_difference)[[3]]` we come to the conclusion that we cannot reject that the mean of the differences is 0. Furthermore, we can not say that the time difference is affected by the type of drink. 

### Task 4

A possible objection could be that the time taken between the two runs are not enough. According to http://www.sciencefocus.com/article/human-body/how-long-does-caffeine-take-kick, caffeine takes around 45 minutes to kick in. So maybe half an hour was not enough to see the effects of the drinks in the running speed.


### Task 5 

Our possible objection could be that 30 minutes is not enough time to rest and do the second run in the same conditions as the first run.


### Task 6
The test assumes that the distribution of the differences is normal.

Here we show the qqplots for the ttme difference for the soft drinks and the energy drinks.
```{r}
qqnorm(lemo_time_difference)
qqline(lemo_time_difference)

qqnorm(energy_time_difference)
qqline(energy_time_difference)
```


## Exercise 7

First, we load the data from the data source.
```{r}
dogs <- read.table("dogs.txt", header = TRUE)
attach(dogs)

```

### Task 1 

First of all we start with boxplots of all the different drugs on the population.
```{r}
boxplot(dogs, names = c("Isofluorane", "Halothane", "Cyclopropane") )

```

Then we check the normality on the distributions of each drug. Showing the qqplots first and then performing the shapiro test.

```{r}
qqnorm(isofluorane, main= "Q-Q Plot of Isofluorane" )
qqline(isofluorane) 
```

```{r, results="hide"}
shapiro.test(isofluorane)

```
It it not reasonable to assume that the Isofluorane data was taken from a normal population, having the p-value `r shapiro.test(isofluorane)[[2]]`.
```{r}
qqnorm(halothane, main= "Q-Q Plot of Halothane" )
qqline(halothane) 
```
```{r, results="hide"}
shapiro.test(halothane)

```
Taking a look at the p-value `r shapiro.test(halothane)[[2]]` we can say it is reasonable to assume that the population of Halothane was taken from a normal population.

```{r}
qqnorm(cyclopropane, main= "Q-Q Plot of Cyclopropane" )
qqline(cyclopropane)
```

```{r, results="hide"}
shapiro.test(cyclopropane)

```
Taking a look at the p-value `r shapiro.test(cyclopropane)[[2]]` we can say it is reasonable to assume that the population of Halothane was taken from a normal population.


### Task 2

```{r, results="hide"}
dogframe = data.frame(plasma=as.vector(as.matrix(dogs)),  group = factor(rep(1:3, each=10)))
dog_aov = lm(plasma~group, data = dogframe)
anova(dog_aov) 

summary(dog_aov)
```
Having the p-value from the anova test being `r anova(dog_aov)[[5]]` we can reject that mu1 = mu2 = mu3. Therefore, the concentration is not the same under the different drugs.

The estimate for Isofluorane is`r summary(dog_aov)[[4]][[1]]` the estimate for Halothane is `r summary(dog_aov)[[4]][[1]] + summary(dog_aov)[[4]][[2]]` and the estimate for Cyclopropane is `r summary(dog_aov)[[4]][[1]] + summary(dog_aov)[[4]][[3]]`.


### Task 3

```{r, results="hide"}
attach(dogframe)
kruskal.test(plasma, group)
```

The test gives us the p-value `r kruskal.test(plasma, group)[[3]]`. Which indicates that we cannot reject that m0 = m1 = m2. So we can say it differs from the conclussion from the anova test.